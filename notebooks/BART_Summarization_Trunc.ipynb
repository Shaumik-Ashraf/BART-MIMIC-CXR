{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART Summarization",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui57cvG9BoX0",
        "outputId": "5a8c8ece-a8e4-43a0-96c7-db766661f890"
      },
      "source": [
        "!pip install transformers==4.5.0\n",
        "!pip install datasets\n",
        "!git clone https://github.com/Shaumik-Ashraf/BART-MIMIC-CXR.git\n",
        "!pip install -r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt\n",
        "import torch\n",
        "from transformers import BartModel, BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.4.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=582973d59ad2f4dbe7336e2a9b020c2ad1a76fc91dea4c17cfa01097c1965127\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/11/f7689b996f85e45f718745c899f6747ee5edb4878cadac0a41ab146828fa/fsspec-0.9.0-py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: huggingface-hub, fsspec, xxhash, datasets\n",
            "Successfully installed datasets-1.5.0 fsspec-0.9.0 huggingface-hub-0.0.8 xxhash-2.0.0\n",
            "Cloning into 'BART-MIMIC-CXR'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 162 (delta 85), reused 99 (delta 38), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (162/162), 96.57 KiB | 5.08 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "Requirement already satisfied: datasets>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (1.5.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 3)) (3.12.4)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 6)) (3.2.5)\n",
            "Collecting py7zr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/00/d59c2c882b53f6a37f9e29705398a4641298b43baa408ad6ebed480860ce/py7zr-0.15.1-py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 3)) (54.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 3)) (1.15.0)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 5)) (0.12.0)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 38.5MB/s \n",
            "\u001b[?25hCollecting bcj-cffi<0.6.0,>=0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/1f/408e7b01375c863e01c25b1475d628deab7c9f85aeb74cced4caa5a512ce/bcj_cffi-0.5.1-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Collecting ppmd-cffi<0.5.0,>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/aa/cbb3bb8a03e4b84eb9ecbfb815e33349ed2bc577cb20b9ea7222d34adb1a/ppmd_cffi-0.4.1-cp37-cp37m-manylinux2014_x86_64.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.5MB/s \n",
            "\u001b[?25hCollecting pyzstd<0.15.0,>=0.14.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/e9/fe897f8bb96163645a5b2d3a60ff8bfa6fcdedff4691a3c6c861b0324ef4/pyzstd-0.14.4-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 28.1MB/s \n",
            "\u001b[?25hCollecting texttable\n",
            "  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n",
            "Collecting multivolumefile<0.3.0,>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/02/2d/c7b951e8624edc8f44e544203cb45e5bad4b493665ecc7e442a6ff6cd943/multivolumefile-0.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: cffi>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from bcj-cffi<0.6.0,>=0.5.1->py7zr->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 7)) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.14.0->bcj-cffi<0.6.0,>=0.5.1->py7zr->-r /content/BART-MIMIC-CXR/transformers/seq2seq/requirements.txt (line 7)) (2.20)\n",
            "Installing collected packages: sentencepiece, portalocker, sacrebleu, rouge-score, pycryptodome, bcj-cffi, ppmd-cffi, pyzstd, texttable, multivolumefile, py7zr\n",
            "Successfully installed bcj-cffi-0.5.1 multivolumefile-0.2.2 portalocker-2.0.0 ppmd-cffi-0.4.1 py7zr-0.15.1 pycryptodome-3.10.1 pyzstd-0.14.4 rouge-score-0.0.4 sacrebleu-1.5.1 sentencepiece-0.1.95 texttable-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAbflrKOtMYf",
        "outputId": "7ddc4c88-7c0d-43f9-f373-c1c6e1bc53b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "directory = '/content/drive/MyDrive/NLP in Health'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3a-yADseiXP"
      },
      "source": [
        "def load_file(filename):\n",
        "\t\"\"\"\n",
        "\tloads csv data and returns it as np matrix\n",
        "\t\n",
        "\tparam: filename - path to csv file\n",
        "\treturn: 2D numpy of csv data with text\n",
        "\t\"\"\"\n",
        "\tprint(f\"Loading data from {filename}...\");\n",
        "\tdf = pd.read_csv(filename)\n",
        "\tprint(f\"Done.\");\n",
        "\treturn( np.array(df) );\n",
        "\n",
        "def load_bart(model_name='facebook/bart-large-cnn', tokenizer_name='facebook/bart-large'):\n",
        "\t\"\"\"\n",
        "\tloads pretrained BART model and tokenizer\n",
        "\t\n",
        "\tparams: model_name - pretrained BART huggingface transformer download path, default: facebook/bart-large-cnn\n",
        "\t\t    tokenizer_name - pretrained BART huggingface tokenizer download path, default: facebook/bart-large\n",
        "\treturn: (model, tokenizer)\n",
        "\t\"\"\"\n",
        "\tprint(f\"Loading pretrained model {model_name}...\");\n",
        "\tmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\tprint(\"Done.\");\n",
        "\tprint(f\"Loading pretrained tokenizer {tokenizer_name}...\");\n",
        "\ttokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "\tprint(\"Done.\");\n",
        "\treturn((model, tokenizer));\n",
        "\n",
        "def baseBart(article_to_summarize, model, tokenizer):\n",
        "\t\"\"\"\n",
        "\truns BART summarization\n",
        "\t\n",
        "\tparams: model - from load_bart()\n",
        "\t\t    tokenizer - from load_bart()\n",
        "\t\t\tarticle_to_summarize - text (string)\n",
        "\treturn: generated abstractive summary (string)\n",
        "\t\"\"\"\n",
        "\tinputs = tokenizer([article_to_summarize], max_length=1024, return_tensors='pt')\n",
        "\tinputs.to(device)\n",
        "\tsummary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=25, early_stopping=True)\n",
        "\treturn [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
        "\n",
        "def write_csv_row(opened_file, row, model, tokenizer):\n",
        "\t\"\"\"\n",
        "\tgenerates abstractive summary and writes it to a file in csv format, 1 summary per row\n",
        "\t\n",
        "\tparams: opened_file - open File object, actually any IO stream implementing write() works\n",
        "\t\t    row - a list/array containing [<subject id>, <study id>, <text to summarize>, <ground truth summary>]\n",
        "\t\t\tmodel - trained BART model\n",
        "\t\t\ttokenizer - BART tokenizer\n",
        "\treturns: generated summary\n",
        "\t\"\"\"\n",
        "\tcomp_summary = baseBart(row[2], model, tokenizer)\n",
        "\topened_file.write(f\"\\\"{row[0]}\\\",\\\"{row[1]}\\\",\\\"{comp_summary}\\\",\\\"{row[3]}\\\"\\n\");\n",
        "\treturn(comp_summary);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBpFAL_Ghyx-"
      },
      "source": [
        "TEST_FILE = '/content/test.csv'\n",
        "LIMIT = -1\n",
        "SUMMARIES_FILE = '/content/drive/MyDrive/NLP in Health/Summaries_' + str(LIMIT) + '.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUnht2RYQ5EM",
        "outputId": "54552a52-b0e7-4d3f-a0f5-e4853f4fc769"
      },
      "source": [
        "print(\"==================== Start abstractive summarization ======================\");\n",
        "\n",
        "data = load_file(TEST_FILE);\n",
        "model, tokenizer = load_bart();\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Writing {os.path.basename(SUMMARIES_FILE)}...\");\n",
        "f = open(SUMMARIES_FILE, 'w');\n",
        "f.write(f\"\\\"subject_id\\\",\\\"study_id\\\",\\\"prediction\\\",\\\"actual\\\"\\n\");\n",
        "i = 0;\n",
        "if LIMIT==-1: # based on the limit, print progress messages appropriately\n",
        "\tfor row in data:\n",
        "\t\twrite_csv_row(f, row, model, tokenizer);\n",
        "\t\tif( (i%1000 == 0) or (i+1 == LIMIT) ):\n",
        "\t\t\tprint(f\"Computed {i+1} summaries\");\n",
        "\t\ti += 1;\n",
        "elif LIMIT < 100:\n",
        "\tfor row in data[:LIMIT]:\n",
        "\t\twrite_csv_row(f, row, model, tokenizer);\n",
        "\t\tif( (i%(int(LIMIT/4)) == 0) or (i+1 == LIMIT)):\n",
        "\t\t\tprint(f\"Computed {i+1} summaries\");\n",
        "\t\ti += 1;\n",
        "else:\n",
        "\tfor row in data[:LIMIT]:\n",
        "\t\twrite_csv_row(f, row, model, tokenizer);\n",
        "\t\tif( (i%(int(LIMIT/8)) == 0) or (i+1 == LIMIT) ):\n",
        "\t\t\tprint(f\"Computed {i+1} summaries\");\n",
        "\t\ti += 1;\n",
        "\n",
        "f.close();\n",
        "print(\"Done.\\n\");\n",
        "print(\"==================== End abstractive summarization ======================\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Start abstractive summarization ======================\n",
            "Loading data from /content/test.csv...\n",
            "Done.\n",
            "Loading pretrained model facebook/bart-large-cnn...\n",
            "Done.\n",
            "Loading pretrained tokenizer facebook/bart-large...\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing Summaries_-1.csv...\n",
            "Computed 1 summaries\n",
            "Computed 1001 summaries\n",
            "Computed 2001 summaries\n",
            "Computed 3001 summaries\n",
            "Computed 4001 summaries\n",
            "Computed 5001 summaries\n",
            "Computed 6001 summaries\n",
            "Computed 7001 summaries\n",
            "Computed 8001 summaries\n",
            "Computed 9001 summaries\n",
            "Computed 10001 summaries\n",
            "Computed 11001 summaries\n",
            "Computed 12001 summaries\n",
            "Done.\n",
            "\n",
            "==================== End abstractive summarization ======================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH1NrHtFRESv",
        "outputId": "c7fbbd48-90d9-40f2-c6ae-342d0ebc3e92"
      },
      "source": [
        "!python /content/BART-MIMIC-CXR/transformers/seq2seq/run_summarization.py --model_name facebook/bart-large-cnn --tokenizer_name facebook/bart-large --output_dir output_small --train_file /content/BART-MIMIC-CXR/data/train_small.csv --validation_file /content/BART-MIMIC-CXR/data/val_small.csv --test_file /content/BART-MIMIC-CXR/data/test_small.csv --text_column findings --summary_column impression --max_source_length 1024 --max_target_length 25 --num_beams 4 --do_train --do_eval --do_predict --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-12 04:20:54.330449: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/12/2021 04:20:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/12/2021 04:20:55 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='output_small', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr12_04-20-55_f15633b10ab8', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='output_small', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "04/12/2021 04:20:55 - WARNING - datasets.builder -   Using custom data configuration default-3172af24b2f32cdb\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-3172af24b2f32cdb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3172af24b2f32cdb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.82b231ae4e2af9a2940054e59ec8063a18f1507c81f479107cffc812f0772189\n",
            "Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "https://huggingface.co/facebook/bart-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqtdndzls\n",
            "Downloading: 100% 1.52k/1.52k [00:00<00:00, 1.81MB/s]\n",
            "storing https://huggingface.co/facebook/bart-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.01119ad5ed0734de7152ef51ba44fccefe008001bca9a6ddebeec1caf28f6bb8\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.01119ad5ed0734de7152ef51ba44fccefe008001bca9a6ddebeec1caf28f6bb8\n",
            "loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.01119ad5ed0734de7152ef51ba44fccefe008001bca9a6ddebeec1caf28f6bb8\n",
            "Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d94f53c8851dcda40774f97280e634b94b721a58e71bcc152b5f51d0d49a046a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
            "loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 88.83ba/s]\n",
            "100% 1/1 [00:00<00:00, 199.27ba/s]\n",
            "100% 1/1 [00:00<00:00, 155.22ba/s]\n",
            "Downloading: 5.61kB [00:00, 4.27MB/s]       \n",
            "***** Running training *****\n",
            "  Num examples = 50\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 21\n",
            "100% 21/21 [00:10<00:00,  2.11it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 10.8172, 'train_samples_per_second': 1.941, 'epoch': 3.0}\n",
            "100% 21/21 [00:10<00:00,  1.95it/s]\n",
            "Saving model checkpoint to output_small\n",
            "Configuration saved in output_small/config.json\n",
            "Model weights saved in output_small/pytorch_model.bin\n",
            "tokenizer config file saved in output_small/tokenizer_config.json\n",
            "Special tokens file saved in output_small/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                      =        3.0\n",
            "  init_mem_cpu_alloc_delta   =      490MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  init_mem_gpu_alloc_delta   =     1550MB\n",
            "  init_mem_gpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =       13MB\n",
            "  train_mem_cpu_peaked_delta =        0MB\n",
            "  train_mem_gpu_alloc_delta  =     4654MB\n",
            "  train_mem_gpu_peaked_delta =     1797MB\n",
            "  train_runtime              = 0:00:10.81\n",
            "  train_samples              =         50\n",
            "  train_samples_per_second   =      1.941\n",
            "04/12/2021 04:21:44 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20\n",
            "  Batch size = 8\n",
            "100% 3/3 [00:02<00:00,  1.02it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_gen_len              =       25.0\n",
            "  eval_loss                 =     2.2841\n",
            "  eval_mem_cpu_alloc_delta  =        2MB\n",
            "  eval_mem_cpu_peaked_delta =        0MB\n",
            "  eval_mem_gpu_alloc_delta  =        0MB\n",
            "  eval_mem_gpu_peaked_delta =      531MB\n",
            "  eval_rouge1               =    33.4626\n",
            "  eval_rouge2               =    19.5592\n",
            "  eval_rougeL               =    30.1765\n",
            "  eval_rougeLsum            =    32.5317\n",
            "  eval_runtime              = 0:00:04.39\n",
            "  eval_samples              =         20\n",
            "  eval_samples_per_second   =      4.548\n",
            "04/12/2021 04:21:48 - INFO - __main__ -   *** Test ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 20\n",
            "  Batch size = 8\n",
            "100% 3/3 [00:02<00:00,  1.22it/s]***** test metrics *****\n",
            "  test_gen_len              =       25.0\n",
            "  test_loss                 =     1.7648\n",
            "  test_mem_cpu_alloc_delta  =        0MB\n",
            "  test_mem_cpu_peaked_delta =        1MB\n",
            "  test_mem_gpu_alloc_delta  =        0MB\n",
            "  test_mem_gpu_peaked_delta =      586MB\n",
            "  test_rouge1               =    35.7517\n",
            "  test_rouge2               =      20.86\n",
            "  test_rougeL               =    31.5752\n",
            "  test_rougeLsum            =    34.0868\n",
            "  test_runtime              = 0:00:04.29\n",
            "  test_samples              =         20\n",
            "  test_samples_per_second   =      4.662\n",
            "100% 3/3 [00:03<00:00,  1.01s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hEQ6P6pUkw9",
        "outputId": "e1d989da-d0fa-4284-9835-0540d04ff784"
      },
      "source": [
        "!python /content/BART-MIMIC-CXR/transformers/seq2seq/run_summarization.py --model_name facebook/bart-large-cnn --tokenizer_name facebook/bart-large --output_dir '/content/drive/MyDrive/NLP in Health/Output' --train_file '/content/BART-MIMIC-CXR/data/train.csv' --validation_file '/content/BART-MIMIC-CXR/data/validation.csv' --test_file '/content/BART-MIMIC-CXR/data/test.csv' --text_column findings --summary_column impression --max_source_length 1024 --max_target_length 25 --num_beams 4 --do_train --do_eval --do_predict --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-12 04:25:46.402949: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/12/2021 04:25:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/12/2021 04:25:47 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/NLP in Health/Output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr12_04-25-47_f15633b10ab8', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/NLP in Health/Output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "04/12/2021 04:25:47 - WARNING - datasets.builder -   Using custom data configuration default-c1f76fa641c057ca\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-c1f76fa641c057ca/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c1f76fa641c057ca/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.82b231ae4e2af9a2940054e59ec8063a18f1507c81f479107cffc812f0772189\n",
            "Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.01119ad5ed0734de7152ef51ba44fccefe008001bca9a6ddebeec1caf28f6bb8\n",
            "Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d94f53c8851dcda40774f97280e634b94b721a58e71bcc152b5f51d0d49a046a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
            "loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "100% 97/97 [00:17<00:00,  5.63ba/s]\n",
            "100% 13/13 [00:02<00:00,  6.44ba/s]\n",
            "100% 13/13 [00:02<00:00,  6.38ba/s]\n",
            "***** Running training *****\n",
            "  Num examples = 96962\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 36363\n",
            "{'loss': 1.5366, 'learning_rate': 4.931248796853945e-05, 'epoch': 0.04}\n",
            "  1% 500/36363 [04:31<4:42:36,  2.12it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.2666, 'learning_rate': 4.862497593707891e-05, 'epoch': 0.08}\n",
            "  3% 1000/36363 [09:59<5:18:23,  1.85it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.182, 'learning_rate': 4.793746390561835e-05, 'epoch': 0.12}\n",
            "  4% 1500/36363 [15:30<5:07:23,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-1500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.1192, 'learning_rate': 4.72499518741578e-05, 'epoch': 0.17}\n",
            "  6% 2000/36363 [21:02<4:49:28,  1.98it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.0871, 'learning_rate': 4.656243984269725e-05, 'epoch': 0.21}\n",
            "  7% 2500/36363 [26:31<4:59:49,  1.88it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-2500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.0482, 'learning_rate': 4.58749278112367e-05, 'epoch': 0.25}\n",
            "  8% 3000/36363 [32:02<4:37:54,  2.00it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 1.0591, 'learning_rate': 4.518741577977615e-05, 'epoch': 0.29}\n",
            " 10% 3500/36363 [37:31<5:03:41,  1.80it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-3500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 1.03, 'learning_rate': 4.44999037483156e-05, 'epoch': 0.33}\n",
            " 11% 4000/36363 [43:03<4:39:20,  1.93it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 1.0181, 'learning_rate': 4.3812391716855045e-05, 'epoch': 0.37}\n",
            " 12% 4500/36363 [48:36<4:46:17,  1.85it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-4500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 1.0086, 'learning_rate': 4.31248796853945e-05, 'epoch': 0.41}\n",
            " 14% 5000/36363 [54:08<4:53:20,  1.78it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 1.0, 'learning_rate': 4.243736765393394e-05, 'epoch': 0.45}\n",
            " 15% 5500/36363 [59:39<4:38:53,  1.84it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-5500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 0.9783, 'learning_rate': 4.1749855622473395e-05, 'epoch': 0.5}\n",
            " 17% 6000/36363 [1:05:12<4:25:59,  1.90it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 0.9672, 'learning_rate': 4.106234359101285e-05, 'epoch': 0.54}\n",
            " 18% 6500/36363 [1:10:39<4:33:33,  1.82it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-6500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 0.9335, 'learning_rate': 4.037483155955229e-05, 'epoch': 0.58}\n",
            " 19% 7000/36363 [1:16:07<4:41:21,  1.74it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-7000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 0.9724, 'learning_rate': 3.9687319528091745e-05, 'epoch': 0.62}\n",
            " 21% 7500/36363 [1:21:42<4:21:14,  1.84it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-7500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 0.9369, 'learning_rate': 3.899980749663119e-05, 'epoch': 0.66}\n",
            " 22% 8000/36363 [1:27:17<4:31:05,  1.74it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-8000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 0.9443, 'learning_rate': 3.831229546517064e-05, 'epoch': 0.7}\n",
            " 23% 8500/36363 [1:32:49<4:26:29,  1.74it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-8500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 0.9237, 'learning_rate': 3.7624783433710094e-05, 'epoch': 0.74}\n",
            " 25% 9000/36363 [1:38:20<4:27:50,  1.70it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-9000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 0.9097, 'learning_rate': 3.693727140224954e-05, 'epoch': 0.78}\n",
            " 26% 9500/36363 [1:43:52<4:03:32,  1.84it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-9500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 0.8885, 'learning_rate': 3.6249759370788985e-05, 'epoch': 0.83}\n",
            " 28% 10000/36363 [1:49:19<3:52:06,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-10000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 0.9206, 'learning_rate': 3.5562247339328444e-05, 'epoch': 0.87}\n",
            " 29% 10500/36363 [1:54:51<3:42:49,  1.93it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-10500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-10500/special_tokens_map.json\n",
            "{'loss': 0.8814, 'learning_rate': 3.487473530786789e-05, 'epoch': 0.91}\n",
            " 30% 11000/36363 [2:00:18<3:44:08,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-11000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11000/special_tokens_map.json\n",
            "{'loss': 0.8753, 'learning_rate': 3.4187223276407335e-05, 'epoch': 0.95}\n",
            " 32% 11500/36363 [2:05:48<3:30:02,  1.97it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-11500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-11500/special_tokens_map.json\n",
            "{'loss': 0.889, 'learning_rate': 3.3499711244946794e-05, 'epoch': 0.99}\n",
            " 33% 12000/36363 [2:11:20<3:43:34,  1.82it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-12000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12000/special_tokens_map.json\n",
            "{'loss': 0.8127, 'learning_rate': 3.281219921348624e-05, 'epoch': 1.03}\n",
            " 34% 12500/36363 [2:16:54<3:52:05,  1.71it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-12500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-12500/special_tokens_map.json\n",
            "{'loss': 0.7936, 'learning_rate': 3.2124687182025685e-05, 'epoch': 1.07}\n",
            " 36% 13000/36363 [2:22:26<3:54:11,  1.66it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-13000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13000/special_tokens_map.json\n",
            "{'loss': 0.7803, 'learning_rate': 3.143717515056514e-05, 'epoch': 1.11}\n",
            " 37% 13500/36363 [2:27:58<3:34:54,  1.77it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-13500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-13500/special_tokens_map.json\n",
            "{'loss': 0.7949, 'learning_rate': 3.074966311910459e-05, 'epoch': 1.16}\n",
            " 39% 14000/36363 [2:33:28<3:21:58,  1.85it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-14000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 0.7772, 'learning_rate': 3.0062151087644035e-05, 'epoch': 1.2}\n",
            " 40% 14500/36363 [2:38:59<3:15:57,  1.86it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-14500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-14500/special_tokens_map.json\n",
            "{'loss': 0.7708, 'learning_rate': 2.9374639056183484e-05, 'epoch': 1.24}\n",
            " 41% 15000/36363 [2:44:28<3:00:35,  1.97it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-15000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 0.7831, 'learning_rate': 2.8687127024722932e-05, 'epoch': 1.28}\n",
            " 43% 15500/36363 [2:50:01<3:03:47,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-15500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-15500/special_tokens_map.json\n",
            "{'loss': 0.774, 'learning_rate': 2.7999614993262385e-05, 'epoch': 1.32}\n",
            " 44% 16000/36363 [2:55:32<3:14:05,  1.75it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-16000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 0.7709, 'learning_rate': 2.7312102961801833e-05, 'epoch': 1.36}\n",
            " 45% 16500/36363 [3:01:02<3:06:02,  1.78it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-16500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-16500/special_tokens_map.json\n",
            "{'loss': 0.759, 'learning_rate': 2.6624590930341282e-05, 'epoch': 1.4}\n",
            " 47% 17000/36363 [3:06:35<2:59:05,  1.80it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-17000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17000/special_tokens_map.json\n",
            "{'loss': 0.778, 'learning_rate': 2.5937078898880728e-05, 'epoch': 1.44}\n",
            " 48% 17500/36363 [3:12:01<2:49:06,  1.86it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-17500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-17500/special_tokens_map.json\n",
            "{'loss': 0.7758, 'learning_rate': 2.5249566867420183e-05, 'epoch': 1.49}\n",
            " 50% 18000/36363 [3:17:34<2:50:52,  1.79it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-18000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18000/special_tokens_map.json\n",
            "{'loss': 0.763, 'learning_rate': 2.456205483595963e-05, 'epoch': 1.53}\n",
            " 51% 18500/36363 [3:23:04<2:37:14,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-18500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-18500/special_tokens_map.json\n",
            "{'loss': 0.7615, 'learning_rate': 2.387454280449908e-05, 'epoch': 1.57}\n",
            " 52% 19000/36363 [3:28:37<2:32:59,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-19000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19000/special_tokens_map.json\n",
            "{'loss': 0.7417, 'learning_rate': 2.318703077303853e-05, 'epoch': 1.61}\n",
            " 54% 19500/36363 [3:34:08<2:48:34,  1.67it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-19500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-19500/special_tokens_map.json\n",
            "{'loss': 0.7583, 'learning_rate': 2.249951874157798e-05, 'epoch': 1.65}\n",
            " 55% 20000/36363 [3:39:43<2:24:56,  1.88it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-20000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 0.7494, 'learning_rate': 2.1812006710117427e-05, 'epoch': 1.69}\n",
            " 56% 20500/36363 [3:45:18<2:10:58,  2.02it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-20500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-20500/special_tokens_map.json\n",
            "{'loss': 0.7569, 'learning_rate': 2.112449467865688e-05, 'epoch': 1.73}\n",
            " 58% 21000/36363 [3:50:51<2:20:14,  1.83it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-21000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21000/special_tokens_map.json\n",
            "{'loss': 0.749, 'learning_rate': 2.0436982647196325e-05, 'epoch': 1.77}\n",
            " 59% 21500/36363 [3:56:22<2:03:00,  2.01it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-21500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-21500/special_tokens_map.json\n",
            "{'loss': 0.7449, 'learning_rate': 1.9749470615735777e-05, 'epoch': 1.82}\n",
            " 61% 22000/36363 [4:02:00<2:13:35,  1.79it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-22000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22000/special_tokens_map.json\n",
            "{'loss': 0.7328, 'learning_rate': 1.9061958584275226e-05, 'epoch': 1.86}\n",
            " 62% 22500/36363 [4:07:37<2:07:16,  1.82it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-22500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-22500/special_tokens_map.json\n",
            "{'loss': 0.7428, 'learning_rate': 1.8374446552814675e-05, 'epoch': 1.9}\n",
            " 63% 23000/36363 [4:13:10<2:00:52,  1.84it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-23000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23000/special_tokens_map.json\n",
            "{'loss': 0.7427, 'learning_rate': 1.7686934521354123e-05, 'epoch': 1.94}\n",
            " 65% 23500/36363 [4:18:44<2:02:37,  1.75it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-23500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-23500/special_tokens_map.json\n",
            "{'loss': 0.7277, 'learning_rate': 1.6999422489893572e-05, 'epoch': 1.98}\n",
            " 66% 24000/36363 [4:24:17<1:51:39,  1.85it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-24000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24000/special_tokens_map.json\n",
            "{'loss': 0.6792, 'learning_rate': 1.6311910458433025e-05, 'epoch': 2.02}\n",
            " 67% 24500/36363 [4:29:51<1:44:24,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-24500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-24500/special_tokens_map.json\n",
            "{'loss': 0.6388, 'learning_rate': 1.5624398426972473e-05, 'epoch': 2.06}\n",
            " 69% 25000/36363 [4:35:22<1:37:45,  1.94it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-25000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25000/special_tokens_map.json\n",
            "{'loss': 0.6166, 'learning_rate': 1.4936886395511924e-05, 'epoch': 2.1}\n",
            " 70% 25500/36363 [4:40:54<1:36:19,  1.88it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-25500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-25500/special_tokens_map.json\n",
            "{'loss': 0.6146, 'learning_rate': 1.4249374364051371e-05, 'epoch': 2.15}\n",
            " 72% 26000/36363 [4:46:28<1:37:15,  1.78it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-26000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26000/special_tokens_map.json\n",
            "{'loss': 0.6069, 'learning_rate': 1.3561862332590821e-05, 'epoch': 2.19}\n",
            " 73% 26500/36363 [4:52:00<1:27:08,  1.89it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-26500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-26500/special_tokens_map.json\n",
            "{'loss': 0.6031, 'learning_rate': 1.287435030113027e-05, 'epoch': 2.23}\n",
            " 74% 27000/36363 [4:57:31<1:24:39,  1.84it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-27000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27000/special_tokens_map.json\n",
            "{'loss': 0.6106, 'learning_rate': 1.218683826966972e-05, 'epoch': 2.27}\n",
            " 76% 27500/36363 [5:03:05<1:20:25,  1.84it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-27500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-27500/special_tokens_map.json\n",
            "{'loss': 0.6095, 'learning_rate': 1.149932623820917e-05, 'epoch': 2.31}\n",
            " 77% 28000/36363 [5:08:43<1:14:25,  1.87it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-28000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28000/special_tokens_map.json\n",
            "{'loss': 0.6071, 'learning_rate': 1.0811814206748618e-05, 'epoch': 2.35}\n",
            " 78% 28500/36363 [5:14:16<1:11:55,  1.82it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-28500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-28500/special_tokens_map.json\n",
            "{'loss': 0.6079, 'learning_rate': 1.0124302175288069e-05, 'epoch': 2.39}\n",
            " 80% 29000/36363 [5:19:50<1:07:57,  1.81it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-29000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29000/special_tokens_map.json\n",
            "{'loss': 0.6111, 'learning_rate': 9.436790143827518e-06, 'epoch': 2.43}\n",
            " 81% 29500/36363 [5:25:25<1:04:18,  1.78it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-29500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-29500/special_tokens_map.json\n",
            "{'loss': 0.601, 'learning_rate': 8.749278112366966e-06, 'epoch': 2.48}\n",
            " 83% 30000/36363 [5:31:02<55:08,  1.92it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-30000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30000/special_tokens_map.json\n",
            "{'loss': 0.5969, 'learning_rate': 8.061766080906415e-06, 'epoch': 2.52}\n",
            " 84% 30500/36363 [5:36:31<55:38,  1.76it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-30500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-30500/special_tokens_map.json\n",
            "{'loss': 0.5854, 'learning_rate': 7.374254049445865e-06, 'epoch': 2.56}\n",
            " 85% 31000/36363 [5:42:04<51:25,  1.74it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-31000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31000/special_tokens_map.json\n",
            "{'loss': 0.5931, 'learning_rate': 6.686742017985315e-06, 'epoch': 2.6}\n",
            " 87% 31500/36363 [5:47:34<48:44,  1.66it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-31500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-31500/special_tokens_map.json\n",
            "{'loss': 0.5907, 'learning_rate': 5.999229986524764e-06, 'epoch': 2.64}\n",
            " 88% 32000/36363 [5:53:10<41:39,  1.75it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-32000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32000/special_tokens_map.json\n",
            "{'loss': 0.6052, 'learning_rate': 5.311717955064214e-06, 'epoch': 2.68}\n",
            " 89% 32500/36363 [5:58:40<35:12,  1.83it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-32500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-32500/special_tokens_map.json\n",
            "{'loss': 0.5984, 'learning_rate': 4.6242059236036636e-06, 'epoch': 2.72}\n",
            " 91% 33000/36363 [6:04:10<31:52,  1.76it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-33000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33000/special_tokens_map.json\n",
            "{'loss': 0.5963, 'learning_rate': 3.936693892143112e-06, 'epoch': 2.76}\n",
            " 92% 33500/36363 [6:09:38<26:14,  1.82it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-33500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-33500/special_tokens_map.json\n",
            "{'loss': 0.5831, 'learning_rate': 3.2491818606825616e-06, 'epoch': 2.81}\n",
            " 94% 34000/36363 [6:15:08<22:46,  1.73it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-34000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34000/special_tokens_map.json\n",
            "{'loss': 0.5898, 'learning_rate': 2.5616698292220113e-06, 'epoch': 2.85}\n",
            " 95% 34500/36363 [6:20:37<17:40,  1.76it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-34500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-34500/special_tokens_map.json\n",
            "{'loss': 0.572, 'learning_rate': 1.8741577977614611e-06, 'epoch': 2.89}\n",
            " 96% 35000/36363 [6:26:07<12:12,  1.86it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-35000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35000/special_tokens_map.json\n",
            "{'loss': 0.5906, 'learning_rate': 1.1866457663009104e-06, 'epoch': 2.93}\n",
            " 98% 35500/36363 [6:31:41<07:21,  1.96it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-35500\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-35500/special_tokens_map.json\n",
            "{'loss': 0.5932, 'learning_rate': 4.991337348403597e-07, 'epoch': 2.97}\n",
            " 99% 36000/36363 [6:37:12<03:15,  1.86it/s]Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output/checkpoint-36000\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-36000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-36000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-36000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/checkpoint-36000/special_tokens_map.json\n",
            "100% 36363/36363 [6:41:29<00:00,  2.02it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 24089.7903, 'train_samples_per_second': 1.509, 'epoch': 3.0}\n",
            "100% 36363/36363 [6:41:29<00:00,  1.51it/s]\n",
            "Saving model checkpoint to /content/drive/MyDrive/NLP in Health/Output\n",
            "Configuration saved in /content/drive/MyDrive/NLP in Health/Output/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP in Health/Output/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/NLP in Health/Output/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/NLP in Health/Output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                      =        3.0\n",
            "  init_mem_cpu_alloc_delta   =      478MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  init_mem_gpu_alloc_delta   =     1550MB\n",
            "  init_mem_gpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =    -2054MB\n",
            "  train_mem_cpu_peaked_delta =     2110MB\n",
            "  train_mem_gpu_alloc_delta  =     4678MB\n",
            "  train_mem_gpu_peaked_delta =     4874MB\n",
            "  train_runtime              = 6:41:29.79\n",
            "  train_samples              =      96962\n",
            "  train_samples_per_second   =      1.509\n",
            "04/12/2021 11:08:15 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 12080\n",
            "  Batch size = 8\n",
            "100% 1510/1510 [35:39<00:00,  1.42s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_gen_len              =       25.0\n",
            "  eval_loss                 =     1.0747\n",
            "  eval_mem_cpu_alloc_delta  =       23MB\n",
            "  eval_mem_cpu_peaked_delta =       13MB\n",
            "  eval_mem_gpu_alloc_delta  =        0MB\n",
            "  eval_mem_gpu_peaked_delta =     1391MB\n",
            "  eval_rouge1               =    46.8036\n",
            "  eval_rouge2               =     33.485\n",
            "  eval_rougeL               =    44.0361\n",
            "  eval_rougeLsum            =    45.2443\n",
            "  eval_runtime              = 0:35:40.84\n",
            "  eval_samples              =      12080\n",
            "  eval_samples_per_second   =      5.643\n",
            "04/12/2021 11:43:56 - INFO - __main__ -   *** Test ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 12069\n",
            "  Batch size = 8\n",
            "100% 1509/1509 [34:57<00:00,  1.34s/it]***** test metrics *****\n",
            "  test_gen_len              =       25.0\n",
            "  test_loss                 =     1.0811\n",
            "  test_mem_cpu_alloc_delta  =        6MB\n",
            "  test_mem_cpu_peaked_delta =       13MB\n",
            "  test_mem_gpu_alloc_delta  =        0MB\n",
            "  test_mem_gpu_peaked_delta =     1341MB\n",
            "  test_rouge1               =    46.5469\n",
            "  test_rouge2               =    32.8639\n",
            "  test_rougeL               =    43.5912\n",
            "  test_rougeLsum            =    44.9264\n",
            "  test_runtime              = 0:35:17.18\n",
            "  test_samples              =      12069\n",
            "  test_samples_per_second   =      5.701\n",
            "100% 1509/1509 [35:16<00:00,  1.40s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAOZ26LQSIeO"
      },
      "source": [
        "import regex as re\n",
        "\n",
        "# Eliminate everything after the last period\n",
        "summary_file = '/content/drive/MyDrive/NLP in Health/Output/test_generations.txt'\n",
        "truncated_file = '/content/drive/MyDrive/NLP in Health/Output/truncated_generations.txt'\n",
        "with open(summary_file, 'r') as f:\n",
        "    summaries = [line.strip() for line in f]\n",
        "\n",
        "truncated_summaries = []\n",
        "for text in summaries:\n",
        "  text = re.sub(\"(?<=[a-zA-Z]\\.)[^.]*$\", \"\", text)\n",
        "  truncated_summaries.append(text)\n",
        "\n",
        "with open(truncated_file, \"w\") as f:\n",
        "  for item in truncated_summaries:\n",
        "    f.write(str(item) + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2roiEezKU69-",
        "outputId": "e642b8bb-583b-44ad-fff5-d246800ce4b8"
      },
      "source": [
        "print(len(summaries))\n",
        "print(len(truncated_summaries))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12069\n",
            "12069\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}